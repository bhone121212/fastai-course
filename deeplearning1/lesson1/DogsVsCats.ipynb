{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dogs vs Cats Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning can be used to use a pre-trained to learn new classes. The lesson uses networks trained on ImageNet to learn a new classifier to predict if an image is a dog or a cat\n",
    "\n",
    "This notebook uses the previously generated TFRecords for data and the VGG16 model as a base to learn new classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import urllib\n",
    "import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We converted the directories into TFRecords in the DataPreparation.ipynb previously so will use them as our data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.expanduser('~/data/DogsVsCats')\n",
    "\n",
    "dev_data_dir = os.path.join(data_dir, 'dev')\n",
    "train_data_dir = os.path.join(data_dir, 'train')\n",
    "test_data_dir = os.path.join(data_dir, 'test1')\n",
    "validation_data_dir = os.path.join(data_dir, 'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dev dataset contains only 10 images and is useful for ensuring network runs correctly. Handy to not have to wait a long time while testing the network and can switch to train and validation once entire architecture is ensured to work as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_record_filenames = os.path.join(data_dir, 'dev.tfrecord')\n",
    "train_record_filenames = glob.glob(os.path.join(data_dir, 'train-*.tfrecord'))\n",
    "validation_record_filenames = glob.glob(os.path.join(data_dir, 'validation-*.tfrecord'))\n",
    "test_record_filenames = os.path.join(data_dir, 'test.tfrecord')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries to map from integer label and string label for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'class_name2id.p'), 'rb') as p:\n",
    "    class_name2id = pickle.load(p)\n",
    "id2class = {v:k for k, v in enumerate(class_name2id)}\n",
    "NUM_CLASSES = len(id2class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to construct out data `input_fn` methods to be used later when passed to the `Experiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_input_fn(filenames, num_epochs=1, batch_size=64, shuffle=False):\n",
    "    \n",
    "    def _input_fn():\n",
    "        def _parse_record(tf_record):\n",
    "            features = {\n",
    "                'image': tf.FixedLenFeature([], dtype=tf.string),\n",
    "                'label': tf.FixedLenFeature([], dtype=tf.int64)\n",
    "            }\n",
    "            record = tf.parse_single_example(tf_record, features)\n",
    "\n",
    "            image_raw = tf.decode_raw(record['image'], tf.float32)\n",
    "            image_raw = tf.reshape(image_raw, shape=(224, 224, 3))\n",
    "\n",
    "            label = tf.cast(record['label'], tf.int32)\n",
    "            label = tf.one_hot(label, depth=NUM_CLASSES)\n",
    "\n",
    "            return { 'image': image_raw }, label\n",
    "        \n",
    "        # For TF dataset blog post, see https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html\n",
    "        dataset = tf.contrib.data.TFRecordDataset(filenames)\n",
    "        dataset = dataset.map(_parse_record)\n",
    "\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=256)\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        features, labels = iterator.get_next()\n",
    "\n",
    "        return features, labels\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg16_model_fn(features, mode, params):\n",
    "    \n",
    "    is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    \n",
    "    with tf.variable_scope('vgg_base'):\n",
    "        # Use a pre-trained VGG16 model and drop off the top layers as we will retrain \n",
    "        # with our own dense output for our custom classes\n",
    "        vgg16_base = tf.contrib.keras.applications.VGG16(include_top=False,\n",
    "                                                         input_shape=(224, 224, 3),\n",
    "                                                         input_tensor=features['image'],\n",
    "                                                         pooling='avg')\n",
    "\n",
    "        # Disable training for all layers to increase speed for transfer learning\n",
    "        # If new classes significantely different from ImageNet, this may be worth leaving as trainable = True\n",
    "#         vgg16_base.trainable = False\n",
    "        for layer in vgg16_base.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        x = vgg16_base.output\n",
    "    \n",
    "    # Add an average pooling, dense and dropout layers on the VGG base\n",
    "    with tf.variable_scope(\"fc\"):\n",
    "#         x = tf.layers.average_pooling2d(vgg16_base,\n",
    "#                                        pool_size=(3, 3),\n",
    "#                                        strides=2) # Taken care of by vgg16 pooling='avg'?\n",
    "        x = tf.contrib.layers.flatten(x)\n",
    "        x = tf.layers.dense(x, units=4096, activation=tf.nn.relu, trainable=is_training, name='fc1')\n",
    "        x = tf.layers.dense(x, units=4096, activation=tf.nn.relu, trainable=is_training, name='fc2')\n",
    "        x = tf.layers.dropout(x, rate=0.5, training=is_training)\n",
    "        \n",
    "    # Finally add a 2 dense layer for class predictions\n",
    "    with tf.variable_scope(\"Prediction\"):\n",
    "        x = tf.layers.dense(x, units=NUM_CLASSES, trainable=is_training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    \n",
    "    logits = vgg16_model_fn(features, mode, params)\n",
    "\n",
    "    scores = tf.nn.softmax(logits)\n",
    "    predicted_class = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    \n",
    "    # Dictionary with label as outcome with greatest probability\n",
    "    # Softmax will provide probabilities of each label\n",
    "    predictions = {\n",
    "        'class': predicted_class,\n",
    "        'probabilities': scores\n",
    "    }\n",
    "    \n",
    "    # Return our EstimatorSpec for predict mode\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        export_outputs = {\n",
    "            'predict_output': tf.estimator.export.PredictOutput(predictions)\n",
    "        }\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=predictions,\n",
    "            export_outputs=export_outputs\n",
    "        )\n",
    "\n",
    "    # Softmax loss\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)\n",
    "    \n",
    "    # Return the EstimatorSpec for training mode\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            learning_rate=params['learning_rate'],\n",
    "            optimizer=tf.train.AdamOptimizer\n",
    "        )\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=predictions,\n",
    "            loss=loss,\n",
    "            train_op=train_op\n",
    "        )\n",
    "    \n",
    "    assert mode == tf.estimator.ModeKeys.EVAL\n",
    "    \n",
    "    # Setup evaluation metrics\n",
    "    eval_metrics = {\n",
    "        'accuracy': tf.metrics.accuracy(\n",
    "            labels=tf.argmax(labels),\n",
    "            predictions=predicted_class,\n",
    "            name='accuracy'\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Return EstimatorSpec for evaluation mode\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions,\n",
    "        loss=loss,\n",
    "        eval_metric_ops=eval_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1216fe4a8>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/DogsVsCats'}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'learning_rate': 2e-3\n",
    "}\n",
    "\n",
    "run_config = tf.contrib.learn.RunConfig()\n",
    "run_config = run_config.replace(model_dir='/tmp/DogsVsCats')\n",
    "\n",
    "dog_cat_estimator = tf.estimator.Estimator(\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    params=params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment = tf.contrib.learn.Experiment(\n",
    "    dog_cat_estimator,\n",
    "    train_input_fn=data_input_fn(train_record_filenames, num_epochs=None, batch_size=10, shuffle=True),\n",
    "    eval_input_fn=data_input_fn(validation_record_filenames, num_epochs=None, batch_size=10),\n",
    "    train_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# experiment.train_and_evaluate()\n",
    "experiment.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
